# SplitInference ğŸ”

A research prototype for protecting inference confidentiality using a multiâ€‘enclave splitâ€‘inference architecture over ODML models.

## ğŸš€ Highlights

- **Split Inference Architecture:** Model inference layers are partitioned into input, intermediate, and output groups across trusted input/output enclaves and an untrusted host, protecting both inference requests and responses while maintainingÂ performance and generality.
- **Robust enclave-based key management:** Each app negotiates a unique encryption key with an Enclave using remote attestation, establishing per-app secure channels to enclaves.
- **Native & Hybrid Implementations on Keystone:** We implemented native (C/C++) and hybrid (Python + enclave C/C++) versions for MobileNetV1 and MobileViT models within the Keystone RISCâ€‘V multiâ€‘enclave TEE.
- **Strong performance:** Experimental evaluation shows inference overheads in the range of 27%â€“430%, significantly better than 142%â€“577% seen in prior work.
- **Python-friendly:** The hybrid split inference architecture supports PyTorch and NumPy within the enclave, enabling seamless deployment of a wide range of ODML models implemented in Python.
- **Peer-reviewed:** The prototype work received the **Best Paper Award** at FMECâ€¯2025.

---

## ğŸ“š Table of Contents

- [Highlights](#-highlights)
- [Table of Contents](#-table-of-contents)
- [Publication](#publication)
- [Architecture](#architecture)
- [Implementation](#implementation)
- [Benchmarks](#benchmarks)
- [Getting Started](#getting-started)


---

## Publication

Yongzhi Wang, Ahsan Habib, "Protect Data Confidentiality for On-device Machine Learning through Split Inference", The 10th International Conference on Fog and Mobile Edge Computing (FMEC 2025)", May 19-22, 2025, Tampa, Florida, USA **(Best Paper Award)**

---

## Architecture

Detailed design includes:

1. **Input Enclave**: decrypts and processes input layers.
2. **Middle Host Layer**: untrusted execution of intermediate layers.
3. **Output Enclave**: final layers inside enclave, encrypts inference output.
4. **Key Management & Secure Communication Protocol:** Â ensureÂ confidentiality between each app and enclave.

This architecture ensures that no attacker observing inputs, intermediate data, or outputs can reconstruct the original inputs or outputs.

---

## Implementation

- **Native mode**: Entire inference pipeline translated into C/C++ and the input and output layers are executed inside Keystone enclavesâ€”used for MobileNetV1 models.
- **Hybrid mode**: Python (with PyTorch & NumPy) executes middle layers on host, while input/output enclaves use C/C++â€”used for MobileViT.\
  This hybrid design allows easy adaptation to complex ODML models commonly implemented in Python.

---

## Benchmarks

We evaluated on MobileNetV1 variants (width multipliers 0.7, 0.5, 0.25) and MobileViTV1â€‘XXS with input image size 224Ã—224:

| Model                | Overhead vs Baseline |
| -------------------- | -------------------- |
| MobileNetV1â€‘0.7â€‘224  | \~71%                |
| MobileNetV1â€‘0.5â€‘224  | \~133%               |
| MobileNetV1â€‘0.25â€‘224 | \~430%               |
| MobileViTV1â€‘XXS      | \~27%                |

Hybrid inference (MobileViT) achieves the lowest overhead while preserving strong security guarantees.

---

## Getting Started

```bash
git clone https://github.com/DrVoayger/SplitInference.git
cd SplitInference
# Follow `docs/INSTALL.md` for dependencies: QEMU, Keystone, Python toolchain
# Build native enclave: compile input/output enclave in C/C++
# Run hybrid version: port Python interpreter, PyTorch, Numpy into enclave workspace
```

Refer to `src/` for  setup instructions and code.

---

## Acknowledgements

Special thanks toÂ CAHSI and Google for funding this project.

---